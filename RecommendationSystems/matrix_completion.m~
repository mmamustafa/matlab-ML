function [Y_complete, U, V, loss, k] = matrix_completion(Y_missing, nf, lambda, lr, delta)
% Matrix completion using Gradient descent method with regularization.
% -------------------------------------------------------------------------
% Default values
if nargin < 5
    delta = 1e-4;   % upper bound for change of loss for stopping
    if nargin < 4
        lr = 5e-2;      % learning rate
        if nargin < 3
            lambda = 1e-2;     % regularization weight
            if nargin < 2
                nf = 1;         % number of features (rank of Y_complete)
            end
        end
    end
end
[nr, nc] = size(Y_missing);
% make sure the output rank is less than the input rank
nf = min([nf, nr, nc]);
% Initialization
rand('seed',1);   randn('seed',1);
U = randn(nr,nf);           % row_feature
V = randn(nc,nf);           % col_feature
u_hat = zeros(size(U));     % temp for gradient
v_hat = zeros(size(V));     % temp for gradient
stop = 0;                   % boolean flag
old_loss = 1e9;             % large number
k = 0;
while ~stop
    dif = U*V' - Y_missing;
    % Gradients
    for i=1:nr
        u_hat(i,:) = nansum(V .* (dif(i,:)'*ones(1,nf)));
    end
    G_U = u_hat + lambda * U;
    for j=1:nc
        v_hat(j,:) = nansum(U .* (dif(:,j)*ones(1,nf)));
    end
    G_V = v_hat + lambda * V
    % Update x and theta
    U = U - lr * G_U;
    V = V - lr * G_V;
    % Compute loss
    loss = (nansum(dif(:).^2) + lambda * (sum(U(:).^2) + sum(V(:).^2))) / 2;
    % stopping condition
    if abs(loss-old_loss) < delta
        stop = 1;
    end
    old_loss = loss;
    k = k+1;
    break
end
Y_complete = U*V';
return